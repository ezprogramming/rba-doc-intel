# Architecture Decisions & Rationale

This document explains key architectural choices in the RBA Document Intelligence Platform, with focus on **why** certain technologies were chosen and **where** they apply.

---

## 1. Embedding Service: transformers vs sentence-transformers

### Decision
Use **`transformers` library directly** instead of `sentence-transformers` for the embedding service.

### Where It Applies
**ONLY** the embedding service (`docker/embedding/app.py`).

**Does NOT apply to**:
- ‚ùå PDF processing
- ‚ùå Text chunking
- ‚ùå Table extraction
- ‚ùå Any other part of the pipeline

### Why transformers?

| Aspect | sentence-transformers | transformers |
|--------|----------------------|--------------|
| **Performance** | 9s per chunk (one-by-one) | 3.6s per chunk (batched) ‚úÖ |
| **Variable-length batching** | ‚ùå Tensor shape mismatch | ‚úÖ Proper padding |
| **Control** | Black box | Explicit control ‚úÖ |
| **Production readiness** | Research/prototype | Production APIs ‚úÖ |

**Technical reason**: `sentence-transformers` doesn't properly pad variable-length sequences in batches, causing tensor shape mismatches like:
```
The size of tensor a (860) must match the size of tensor b (846)
```

**Solution**: `transformers` gives explicit control over tokenization:
```python
tokenizer(texts, padding='longest', truncation=True, max_length=8192)
```

This ensures all tensors have uniform shape within a batch.

### Performance Impact
- **2.5x faster** embedding generation
- CPU: 0.28 chunks/sec (vs 0.11 chunks/sec)
- Total corpus time: 2.6 hours (vs 6.5 hours)

### References
- [EMBEDDING_SERVICE_ARCHITECTURE.md](./EMBEDDING_SERVICE_ARCHITECTURE.md) - Complete technical details
- [FAST_EMBEDDING_SOLUTION.md](./FAST_EMBEDDING_SOLUTION.md) - Performance comparison

---

## 2. PDF Processing: PyMuPDF (not transformers)

### Decision
Use **PyMuPDF (`fitz`)** for PDF text extraction.

### Why NOT transformers?
Transformers are ML models for:
- Text embeddings
- Text generation
- Classification
- NER, etc.

They **cannot**:
- ‚ùå Parse binary PDF files
- ‚ùå Extract text from PDF pages
- ‚ùå Handle PDF structure (pages, fonts, images)

### Why PyMuPDF?
- ‚úÖ Fast C++ implementation
- ‚úÖ Reliable Unicode support
- ‚úÖ Handles complex PDFs (multi-column, tables, images)
- ‚úÖ Good API for page-by-page processing
- ‚úÖ Memory-efficient streaming

### Alternative Considered
**pdfplumber**: Better for table extraction but slower for text.

**Our choice**: PyMuPDF for text, Camelot for tables (specialized tool).

---

## 3. Text Chunking: Simple String Operations (not transformers)

### Decision
Use **paragraph-aware recursive splitting** with Python string operations.

### Why NOT transformers?
Chunking is about:
- Splitting text at paragraph/sentence boundaries
- Controlling chunk size (tokens/chars)
- Creating overlaps for context

This is **pure string manipulation**, not ML.

### Our Implementation (`app/pdf/chunker.py`)
```python
def chunk_pages(clean_pages, max_tokens=768, overlap_pct=0.15):
    # 1. Concatenate pages
    full_text = " ".join(clean_pages)

    # 2. Find paragraph boundaries
    boundary = text.find('\n\n', target_pos)

    # 3. Split at boundaries
    chunk_text = full_text[start:end]

    # 4. Add sentence-based overlap
    overlap = get_sentence_overlap(chunk_text, num_sentences=2)
```

**Key features**:
- ‚úÖ Paragraph-aware (preserves semantic units)
- ‚úÖ Sentence-based overlap (maintains context)
- ‚úÖ Table-aware (detects table markers)
- ‚úÖ Fast (no ML inference needed)

---

## 4. Table Extraction: Camelot (not transformers)

### Decision
Use **Camelot** for structured table extraction from PDFs.

### Why NOT transformers?
Table extraction from PDFs requires:
- PDF rendering (detect lines, borders)
- Geometric analysis (find table boundaries)
- Cell detection (intersection of lines)
- Text alignment (assign text to cells)

This is **computer vision on PDF graphics**, not NLP.

### Why Camelot?
- ‚úÖ Specialized for PDF table extraction
- ‚úÖ Two methods: lattice (bordered tables) + stream (borderless)
- ‚úÖ Returns structured data (rows, columns)
- ‚úÖ Accuracy scores (confidence metrics)

### Our Implementation
```python
# Try lattice first (bordered tables)
tables = camelot.read_pdf(pdf_path, flavor='lattice', pages='all')

# Fallback to stream (borderless tables)
if not tables:
    tables = camelot.read_pdf(pdf_path, flavor='stream', pages='all')

# Extract structured data
for table in tables:
    structured_data = table.df.to_dict('records')  # List of row dicts
```

**Why this approach?**
- Lattice: Fast, accurate for bordered tables
- Stream: Slower but handles borderless tables
- Both return structured data (not just text)

---

## 5. When to Use ML vs Traditional Methods

### Use ML (transformers, LLMs) When:
- ‚úÖ Need semantic understanding (embeddings, similarity)
- ‚úÖ Generate natural language (LLM responses)
- ‚úÖ Classification/NER (document type, entities)
- ‚úÖ Complex reasoning (RAG query answering)

### Use Traditional Methods When:
- ‚úÖ Parsing structured formats (PDF, JSON, XML)
- ‚úÖ String manipulation (chunking, cleaning)
- ‚úÖ Deterministic operations (regex, boundary detection)
- ‚úÖ Performance-critical paths (no inference overhead)

### Our Choices

| Task | Method | Reason |
|------|--------|--------|
| **Embed text** | transformers (ML) | Need semantic vectors |
| **Parse PDF** | PyMuPDF (traditional) | Structured format parsing |
| **Chunk text** | String ops (traditional) | Deterministic splitting |
| **Extract tables** | Camelot (computer vision) | Geometric analysis |
| **Search chunks** | pgvector (ML + traditional) | Hybrid: vectors + full-text |
| **Generate answers** | LLM (ML) | Natural language generation |

---

## 6. Database: PostgreSQL + pgvector (not specialized vector DB)

### Decision
Use **PostgreSQL with pgvector extension** instead of specialized vector databases (Qdrant, Milvus, Weaviate).

### Why?
- ‚úÖ **Single source of truth**: Metadata + vectors + chat logs in one place
- ‚úÖ **ACID transactions**: Consistency between chunks and embeddings
- ‚úÖ **Mature ecosystem**: Backups, replication, monitoring tools
- ‚úÖ **Cost-effective**: No additional service to maintain
- ‚úÖ **Good enough performance**: HNSW index provides 10-100x speedup

### When to Consider Specialized Vector DB?
- üî¥ **Scale**: >10M vectors (Postgres starts to struggle)
- üî¥ **Complex queries**: Multi-vector search, ANN with filters
- üî¥ **Real-time updates**: High-frequency vector insertions

**Our scale**: ~3K chunks ‚Üí PostgreSQL is perfect.

---

## 7. LLM: Local (Ollama) vs Cloud (OpenAI)

### Decision
Use **local LLM via Ollama** (qwen2.5:1.5b) for RAG answers.

### Why Local?
- ‚úÖ **Privacy**: No data sent to external APIs
- ‚úÖ **Cost**: No per-token charges
- ‚úÖ **Latency**: No network overhead
- ‚úÖ **Control**: Can fine-tune on feedback

### Tradeoffs
- ‚ùå **Quality**: Cloud models (GPT-4) are better
- ‚ùå **Hardware**: CPU inference slower than GPU (optimized for 1.5B model)
- ‚ùå **Maintenance**: Model updates require manual download

### When to Use Cloud?
- Production deployment with high quality requirements
- No GPU available
- Budget for API costs ($0.01-0.03 per 1K tokens)

**Our choice**: Local for learning/development, easy to swap for production.

---

## 8. Parallelism: Client-side vs Server-side

### Decision
Use **client-side parallelism** (concurrent HTTP requests) instead of server-side model batching.

### Why?
With `transformers` and proper padding, we can now use **both**:

**Client-side** (4 parallel requests):
- ‚úÖ Better CPU utilization (multi-core)
- ‚úÖ Fault isolation (one failure doesn't block others)
- ‚úÖ Load distribution (with multiple servers)

**Server-side** (batch_size=16):
- ‚úÖ GPU efficiency (parallel tensor ops)
- ‚úÖ Reduced HTTP overhead
- ‚úÖ Better throughput per server

**Our config**:
```bash
EMBEDDING_BATCH_SIZE=16          # Server processes 16 at once
EMBEDDING_PARALLEL_BATCHES=4     # Client sends 4 concurrent requests
Total: 16 √ó 4 = 64 chunks in flight
```

### References
- [PARALLEL_PROCESSING.md](./PARALLEL_PROCESSING.md) - Complete parallelism guide

---

## Summary

| Component | Technology | ML or Traditional? | Reason |
|-----------|-----------|-------------------|--------|
| **Embedding** | transformers | ü§ñ ML | Need semantic vectors |
| **PDF parsing** | PyMuPDF | üìÑ Traditional | Structured format |
| **Chunking** | String ops | üìÑ Traditional | Deterministic |
| **Tables** | Camelot | üëÅÔ∏è Computer Vision | Geometric analysis |
| **Search** | pgvector | ü§ñ ML + üìÑ Traditional | Hybrid approach |
| **LLM** | Ollama | ü§ñ ML | Text generation |

**Key insight**: Use the **simplest tool for the job**. Don't use ML when traditional methods work better.

---

## Further Reading

- [EMBEDDING_SERVICE_ARCHITECTURE.md](./EMBEDDING_SERVICE_ARCHITECTURE.md) - Why transformers
- [FAST_EMBEDDING_SOLUTION.md](./FAST_EMBEDDING_SOLUTION.md) - Performance comparison
- [PARALLEL_PROCESSING.md](./PARALLEL_PROCESSING.md) - Parallelism strategies
- [LEARN.md](../LEARN.md) - Complete code walkthrough
