# Copy this file to .env and adjust as needed. All uncommented keys are required.

POSTGRES_USER=YOUR_POSTGRES_USER
POSTGRES_PASSWORD=YOUR_POSTGRES_PASSWORD
POSTGRES_DB=YOUR_POSTGRES_DB
POSTGRES_HOST=postgres
POSTGRES_PORT=5432

DATABASE_URL=postgresql+psycopg://YOUR_POSTGRES_USER:YOUR_POSTGRES_PASSWORD@postgres:5432/YOUR_POSTGRES_DB
POSTGRES_DSN=postgresql://YOUR_POSTGRES_USER:YOUR_POSTGRES_PASSWORD@postgres:5432/YOUR_POSTGRES_DB

MINIO_ENDPOINT=minio:9000
MINIO_ACCESS_KEY=YOUR_MINIO_ACCESS_KEY
MINIO_SECRET_KEY=YOUR_MINIO_SECRET_KEY
MINIO_SECURE=0
MINIO_BUCKET_RAW_PDF=YOUR_RAW_BUCKET
MINIO_BUCKET_DERIVED=YOUR_DERIVED_BUCKET

# Embeddings configuration (uses LiteLLM - same Ollama container as LLM)
# Format: provider/model (e.g., ollama/nomic-embed-text, text-embedding-3-small)
# Ollama examples: ollama/nomic-embed-text, ollama/mxbai-embed-large
# OpenAI examples: text-embedding-3-small, text-embedding-ada-002
# For CPU-only (Mac/Docker): Use batch_size=6-8, timeout=240-300s
# For GPU: Can use batch_size=16-32, timeout=120s
EMBEDDING_MODEL_NAME=ollama/nomic-embed-text
EMBEDDING_API_BASE_URL=http://llm:11434
EMBEDDING_BATCH_SIZE=8
EMBEDDING_API_TIMEOUT=240
EMBEDDING_PARALLEL_BATCHES=2

# PDF processing configuration
# batch_size: Number of documents to fetch per iteration
# max_workers: Parallel worker threads (2-4 recommended for I/O-bound tasks)
PDF_BATCH_SIZE=16
PDF_MAX_WORKERS=2

# Table extraction configuration
# batch_size: Documents to process per iteration
# max_workers: Parallel processes (4-8 recommended, CPU-intensive)
TABLE_BATCH_SIZE=16
TABLE_MAX_WORKERS=4

# LLM configuration (uses LiteLLM for unified provider access)
# Format: provider/model (e.g., ollama/qwen2.5:7b, openai/gpt-4, anthropic/claude-3-sonnet)
# Ollama examples: ollama/qwen2.5:7b, ollama/llama3, ollama/mistral
# OpenAI examples: gpt-4, gpt-3.5-turbo (no prefix needed)
# Anthropic examples: anthropic/claude-3-sonnet-20240229
LLM_MODEL_NAME=ollama/qwen2.5:7b
LLM_API_BASE_URL=http://llm:11434
LLM_API_KEY=

# ========================================
# Retrieval & Ranking Enhancements (Phase 6)
# ========================================

# MMR (Maximal Marginal Relevance) for result diversity
USE_MMR=1                          # Enable MMR to reduce redundant chunks (default: enabled)
MMR_LAMBDA=0.5                     # 0=max diversity, 1=max relevance

# Ranking strategy
USE_RRF=0                          # Use Reciprocal Rank Fusion instead of weighted combination
USE_RERANKING=0                    # Enable cross-encoder reranking (+25-40% accuracy, ~300ms)

# Hybrid search weights (used when USE_RRF=0)
SEMANTIC_WEIGHT=0.7                # Semantic (vector) search weight
LEXICAL_WEIGHT=0.3                 # Lexical (full-text) search weight
RECENCY_WEIGHT=0.25                # Recency bias weight
TABLE_BOOST_DATA_QUERIES=0.5      # Boost for table chunks when data query detected

# Cross-encoder reranking configuration
# RERANKER_MODEL_NAME=cross-encoder/ms-marco-MiniLM-L-6-v2  # Optional: override default
# RERANKER_DEVICE=mps  # Optional: cpu, cuda, mps, or auto-detect if not set
# RERANKER_BATCH_SIZE=32  # Optional: batch size for reranking
RERANK_MULTIPLIER=10               # Retrieve N*multiplier candidates for reranking

# Context management
MAX_CONTEXT_TOKENS=6000            # Token budget for LLM context window
CHUNK_QUALITY_THRESHOLD=0.5        # Min quality score for chunks (0.0-1.0)

STREAMLIT_SERVER_PORT=8501
# Accepts comma separated years, ranges (2022-2024), or open ended (2023+)
CRAWLER_YEAR_FILTER=
